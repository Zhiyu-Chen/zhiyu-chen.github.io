---
title: Zhiyu's Blog
date: '2019-12-19T07:30:51.000Z'
tags:
  - BERT
  - Paper Reading
  - BERT
  - Explain
---

		<p>The paper specifically studies the ability of attention heads(of BERT-like models) that can recover syntactic dependency relations.</p>
<h3>Method 1: <span class="fontstyle0">Maximum Attention Weights (M</span><span class="fontstyle0">AX</span><span class="fontstyle0">)</span></h3>
<p><span class="fontstyle0">For a given token A, a token B that has the highest attention weight with respect to the token A should be related to token A. A relation is assigned to <img src="https://s0.wp.com/latex.php?latex=%28w_i%2C+w_j%29&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=%28w_i%2C+w_j%29&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=%28w_i%2C+w_j%29&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="(w_i, w_j)" class="latex"> such that&nbsp;</span><img src="https://s0.wp.com/latex.php?latex=j+%3D+%5Carg%5Cmax+W%5Bi%5D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=j+%3D+%5Carg%5Cmax+W%5Bi%5D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=j+%3D+%5Carg%5Cmax+W%5Bi%5D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="j = \arg\max W[i]" class="latex">&nbsp; for each row i where <img src="https://s0.wp.com/latex.php?latex=W+%5Cin+%280%2C1%29%5E%7BT+%5Ctimes+T%7D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002" srcset="https://s0.wp.com/latex.php?latex=W+%5Cin+%280%2C1%29%5E%7BT+%5Ctimes+T%7D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002 1x, https://s0.wp.com/latex.php?latex=W+%5Cin+%280%2C1%29%5E%7BT+%5Ctimes+T%7D&amp;bg=ffffff&amp;fg=222222&amp;s=0&amp;c=20201002&amp;zoom=4.5 4x" alt="W \in (0,1)^{T \times T}" class="latex"> is the attention weights of a head at some layer. Such dependency relations are extracted from all heads at all layers, and the maximum <span class="fontstyle0">undirected unlabeled attachment scores (UUAS)</span>&nbsp; are used over all relation types.</p>
<p>&nbsp;</p>
<h3>Method 2: <span class="fontstyle0">Maximum Spanning Tree (MST)</span></h3>
<p>In the 1st method, directions are not considered since the formed graphs are not valid trees. <span class="fontstyle0">To extract complete valid dependency trees from an attention weight matrix, the method in ” To extract complete valid dependency trees”&nbsp; is used.&nbsp; The attention matrix is treated as a complete weighted directed graph, with the edges pointing from the output token to each attended token. The root of the gold dependency tree is used as the starting node and the <strong>Chu-Liu-Edmonds algorithm</strong> is used to compute the maximum spanning tree.&nbsp; Evaluation method is the same with ” A structural probe for finding syntax in word representations”.<br>
</span></p>
<h3>Experimental Setup</h3>
<p>Special tokens like <span class="fontstyle0">[CLS]</span><span class="fontstyle1">, </span><span class="fontstyle0">[SEP]</span><span class="fontstyle1">, </span><span class="fontstyle0">&lt;s&gt;</span><span class="fontstyle1">, </span><span class="fontstyle0">&lt;/s&gt; are excluded in order to focus on inter-word attention. English Parallel Universal Dependencies (PUD) treebank from the CoNLL 2017 shared task is used. The tokenization of parsed corpus may not match with model’s tokenization. For such cases, non-matching tokens with corresponding attention weights are merged until compatible.<br>
</span></p>
<h3>Baselines</h3>
<p>Since many dependency relations tend to occur in specific positions relative to the parent word,&nbsp; <span class="fontstyle0">the most common positional offset between a parent and child word for a given dependency relation </span>is used as a baseline.&nbsp; For MST, a <span class="fontstyle0">right-branching dependency tree</span> is used as a baseline.&nbsp; A BERT-large model with <strong>randomly initialized weights</strong> is used.</p>
<h3>Results</h3>
<p>&nbsp;</p>
<p><span class="fontstyle0">Figure </span><span class="fontstyle0">2 </span><span class="fontstyle0">and Table </span><span class="fontstyle0">1 </span><span class="fontstyle0">describe the accuracy for the most frequent relation types based on the M</span><span class="fontstyle0">AX </span><span class="fontstyle0">method.&nbsp; BERT models fine-tuned on CoLA MNLI are also tested. MNLI-BERT has a tendency to track clausal dependencies and outperforms others for long-distance advcl <span class="fontstyle2">and </span>csubj <span class="fontstyle2">dependency types. The non-random models outperform random BERT substantially for all dependency types. </span><br>
</span></p>
<p><span class="fontstyle0"><img data-attachment-id="126" data-permalink="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/max_rs2/" data-orig-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png" data-orig-size="666,375" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="max_rs2" data-image-description="" data-image-caption="" data-medium-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=300" data-large-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=666" class="  wp-image-126 aligncenter" src="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=488&amp;h=275" alt="max_rs2" width="488" height="275" srcset="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=488&amp;h=275 488w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=150&amp;h=84 150w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png?w=300&amp;h=169 300w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs2.png 666w" sizes="(max-width: 488px) 100vw, 488px"></span></p>
<p><img data-attachment-id="124" data-permalink="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/max_rs/" data-orig-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png" data-orig-size="1297,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="max_rs" data-image-description="" data-image-caption="" data-medium-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=300" data-large-file="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=720" class="alignnone size-full wp-image-124" src="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=720" alt="max_rs" srcset="https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png 1297w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=150&amp;h=46 150w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=300&amp;h=92 300w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=768&amp;h=234 768w, https://zhiyuchen.com/wp-content/uploads/2019/12/max_rs.png?w=1024&amp;h=313 1024w" sizes="(max-width: 1297px) 100vw, 1297px"></p>
<p><span class="fontstyle0">Figure </span><span class="fontstyle0">3 </span><span class="fontstyle0">shows the accuracy for </span><span class="fontstyle2">nsubj, obj, advmod</span><span class="fontstyle0">, and </span><span class="fontstyle2">amod </span><span class="fontstyle0">relations extracted based on the MST method.</span></p>
<p><img data-attachment-id="127" data-permalink="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/mst_rs/" data-orig-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png" data-orig-size="664,446" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mst_rs" data-image-description="" data-image-caption="" data-medium-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=300" data-large-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=664" class="  wp-image-127 aligncenter" src="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=383&amp;h=257" alt="mst_rs" width="383" height="257" srcset="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=383&amp;h=257 383w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=150&amp;h=101 150w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png?w=300&amp;h=202 300w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs.png 664w" sizes="(max-width: 383px) 100vw, 383px"></p>
<p><span class="fontstyle0">Figure </span><span class="fontstyle0">4 </span><span class="fontstyle0">describes the maximum undirected unlabeled attachment scores (UUAS) across each layer. Although the trained models perform better than the rightbranching baseline in most cases, the performance gap is not substantial. Given that the MST method<br>
uses the root of the gold trees, whereas the rightbranching baseline does not, this implies that <strong>the attention weights in the different layers/heads of the BERT models do not appear to correspond to complete, syntactically informative parse trees. </strong><br>
</span><br>
<img data-attachment-id="128" data-permalink="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/mst_rs2/" data-orig-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png" data-orig-size="662,532" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="mst_rs2" data-image-description="" data-image-caption="" data-medium-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=300" data-large-file="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=662" loading="lazy" class="  wp-image-128 aligncenter" src="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=368&amp;h=296" alt="mst_rs2" width="368" height="296" srcset="https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=368&amp;h=296 368w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=150&amp;h=121 150w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png?w=300&amp;h=241 300w, https://zhiyuchen.com/wp-content/uploads/2019/12/mst_rs2.png 662w" sizes="(max-width: 368px) 100vw, 368px"></p>
<div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">共享此文章：</h3><div class="sd-content"><ul><li class="share-twitter"><a rel="nofollow noopener noreferrer" data-shared="sharing-twitter-120" class="share-twitter sd-button share-icon" href="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/?share=twitter" target="_blank" aria-labelledby="sharing-twitter-120">
				<span id="sharing-twitter-120" hidden="">Click to share on X (Opens in new window)</span>
				<span>X</span>
			</a></li><li class="share-facebook"><a rel="nofollow noopener noreferrer" data-shared="sharing-facebook-120" class="share-facebook sd-button share-icon" href="https://zhiyuchen.com/2019/12/19/paper-reading-do-attention-heads-in-bert-track-syntactic-dependencies/?share=facebook" target="_blank" aria-labelledby="sharing-facebook-120">
				<span id="sharing-facebook-120" hidden="">Click to share on Facebook (Opens in new window)</span>
				<span>Facebook</span>
			</a></li><li class="share-end"></li></ul></div></div></div><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" id="like-post-wrapper-122293174-120-693771deac5e0" data-src="//widgets.wp.com/likes/index.html?ver=20251209#blog_id=122293174&amp;post_id=120&amp;origin=zhiyusite.wordpress.com&amp;obj_id=122293174-120-693771deac5e0&amp;domain=zhiyuchen.com" data-name="like-post-frame-122293174-120-693771deac5e0" data-title="Like or Reblog"><div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></div><span class="sd-text-color"></span><a class="sd-link-color"></a></div>
<div id="jp-relatedposts" class="jp-relatedposts">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
</div></div>			
